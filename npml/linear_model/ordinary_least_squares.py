"""
æ™®é€šæœ€å°äºŒä¹˜æ³•

ç›¸å…³ç†è®º ğŸ‘‰ https://blog.csdn.net/xiligey1/article/details/81369713
"""

import numpy as np
from numpy import ndarray as array
from numpy.linalg import inv, pinv, LinAlgError

from npml.model import Regressor


class OrdinaryLeastSquares(Regressor):

    def fit(self, x_train: array, y_train: array, method: str = "mat", batch_size: int = 32,
            num_iter: int = 500) -> None:
        """è®­ç»ƒ
        :param x_train: (n_samples, n_features), è®­ç»ƒæ•°æ®
        :param y_train: (n_samples,), è®­ç»ƒæ•°æ®çš„å€¼
        :param method: str, è®¡ç®—æ™®é€šæœ€å°äºŒä¹˜æ³•çš„æ–¹æ³•ï¼Œå¯é€‰matã€gdã€gdbã€sgdï¼Œåˆ†åˆ«å¯¹åº”çŸ©é˜µæ³•ã€æ¢¯åº¦ä¸‹é™æ³•ã€æ‰¹é‡æ¢¯åº¦ä¸‹é™æ³•ã€éšæœºæ¢¯åº¦ä¸‹é™æ³•
        :param batch_size: int, æ‰¹é‡æ¢¯åº¦ä¸‹é™æ³•çš„æ‰¹å¤§å°
        :param num_iter: int, æ¢¯åº¦ä¸‹é™æ—¶çš„è¿­ä»£æ¬¡æ•°
        """
        n_samples, n_features = x_train.shape

        # ç»™train_featuresæ·»åŠ ä¸€åˆ—1(ä»£è¡¨å¸¸æ•°é¡¹)ï¼Œå°†train_valuesè½¬æ¢æˆ(n_samples, 1)
        x_train = np.concatenate((np.ones(n_samples).reshape((n_samples, 1)), x_train), axis=1)
        y_train = y_train.reshape((n_samples, 1))

        if method == "mat":
            self._fit_with_matrix(x_train, y_train)
        elif method == "gd":
            self._fit_with_gradient_descent(x_train, y_train, num_iter)
        elif method == "dgb":
            self._fit_with_gradient_descent_batch(x_train, y_train, num_iter, batch_size)
        elif method == "sgd":
            self._fit_with_gradient_descent_random(x_train, y_train, num_iter)
        else:
            raise ValueError(f"method {method} is not supported, please use one of 'mat', 'gd', 'gdb', 'sgd'")

    def _fit_with_matrix(self, x_train: array, y_train: array) -> None:
        """ä½¿ç”¨çŸ©é˜µè®¡ç®—å¾—å‡ºæ™®é€šæœ€å°äºŒä¹˜æ³•çš„æˆªè·é¡¹å’Œç³»æ•°
        :param x_train: (n_samples + 1, n_features), è®­ç»ƒæ•°æ®æ·»åŠ ä¸€åˆ—å¸¸æ•°é¡¹ï¼ˆä¼´éšçŸ©é˜µï¼‰
        :param y_train: (n_samples, 1), è®­ç»ƒæ•°æ®çš„å€¼
        :exception  å¦‚æœæ— æ³•è®¡ç®—é€†çŸ©é˜µï¼Œåˆ™æ•è·LinAlgErrorå¼‚å¸¸ï¼Œç„¶åè®¡ç®—ä¼ªé€†çŸ©é˜µ
        """
        # inverse = (X.T * X)^{-1}
        # A @ B == np.dot(A, B)
        x_train_square = x_train.T @ x_train
        try:
            inverse = inv(x_train_square)
        except LinAlgError as e:
            print(e)
            inverse = pinv(x_train_square)
        theta = inverse @ x_train.T @ y_train
        self.intercept, self.coeffs = theta[0, 0], theta[1:, 0]

    def _fit_with_gradient_descent(self, x_train, y_train, num_iter):
        """æ¢¯åº¦ä¸‹é™æ³•"""

    def _fit_with_gradient_descent_batch(self, x_train, y_train, num_iter, batch_size):
        """æ‰¹é‡æ¢¯åº¦ä¸‹é™æ³•"""

    def _fit_with_gradient_descent_random(self, x_train, y_train, num_iter):
        """éšæœºæ¢¯åº¦ä¸‹é™æ³•"""

    def predict(self, test_features):
        """é¢„æµ‹
        :param test_features: (n_samples, n_features), æµ‹è¯•æ•°æ®
        :return (n_samples,), æµ‹è¯•æ•°æ®çš„é¢„æµ‹å€¼
        pred = X * coeffs + intercept
        """
        return test_features @ self.coeffs + self.intercept
