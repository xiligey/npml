"""
æ™®é€šæœ€å°äºŒä¹˜æ³•
æ”¯æŒçŸ©é˜µæ³•ã€æ‰¹é‡æ¢¯åº¦ä¸‹é™æ³•ã€å°æ‰¹é‡æ¢¯åº¦ä¸‹é™æ³•ã€éšæœºæ¢¯åº¦ä¸‹é™æ³•å››ç§è®¡ç®—æ–¹å¼

ç›¸å…³ç†è®º ðŸ‘‰ https://blog.csdn.net/xiligey1/article/details/81369713
"""
import matplotlib.pyplot as plt
import numpy as np
from numpy import ndarray as array
from numpy.linalg import inv, pinv, LinAlgError

from npml.model import Regressor
from npml.utils.arrays import transform_array
from npml.utils.exceptions import NotFittedWithGradientDescentError
from npml.utils.metrics.regression import mse


class OrdinaryLeastSquares(Regressor):

    def __init__(self):
        super().__init__()
        # æ¨¡åž‹çš„ç‰¹å¾æ•°é‡
        self.n_features = None
        # è®°å½•è¿­ä»£è¿‡ç¨‹çš„æŸå¤±å€¼
        self.losses = None

    def fit(self, x_train: array, y_train: array, method: str = "mat", alpha: float = 0.01,
            batch_size: int = 32, num_iters: int = 500) -> None:
        """è®­ç»ƒ
        :param x_train: (n_samples, n_features), è®­ç»ƒæ•°æ®
        :param y_train: (n_samples,), è®­ç»ƒæ•°æ®çš„å€¼
        :param method: str, è®¡ç®—OLSçš„æ–¹æ³•ï¼Œå¯é€‰matã€gdã€gdbã€sgdï¼Œåˆ†åˆ«å¯¹åº”çŸ©é˜µæ³•ã€æ‰¹é‡æ¢¯åº¦ä¸‹é™æ³•ã€å°æ‰¹é‡æ¢¯åº¦ä¸‹é™æ³•ã€éšæœºæ¢¯åº¦ä¸‹é™æ³•
        :param alpha: float, å­¦ä¹ çŽ‡
        :param batch_size: int, æ‰¹é‡æ¢¯åº¦ä¸‹é™æ³•çš„æ‰¹å¤§å°
        :param num_iters: int, æ¢¯åº¦ä¸‹é™æ—¶çš„è¿­ä»£æ¬¡æ•°
        """
        n_samples, n_features = x_train.shape
        self.n_features = n_features

        # è®¾è®¡çŸ©é˜µï¼Œåœ¨åŽŸæœ‰æ•°æ®çš„æœ€å·¦ä¾§æ·»åŠ ä¸€åˆ—1
        x_train_design = transform_array(x_train)
        y_train = y_train.reshape((n_samples, 1))

        if method == "mat":
            self._fit_with_matrix(x_train_design, y_train)
        else:
            self.losses = []
            self.theta = np.zeros((self.n_features + 1, 1))
            if method == "gd":
                gd_func = self.gradient_descent_batch_func
            elif method == "gdb":
                gd_func = self.gradient_descent_mini_batch_func
            elif method == "sgd":
                gd_func = self.gradient_descent_mini_batch_func
                batch_size = 1
            else:
                raise ValueError(f"method {method} is not supported, please use one of 'mat', 'gd', 'gdb', 'sgd'")
            for _ in range(num_iters):
                self.theta = gd_func(*[x_train_design, y_train, self.theta, alpha, batch_size])
                y_train_pred = (x_train_design @ self.theta).flatten()
                # æŸå¤±å‡½æ•°ä¸ºmse/2
                self.losses.append(mse(y_train, y_train_pred) / 2)
            self.losses = np.asarray(self.losses)

    def _fit_with_matrix(self, x_train_design: array, y_train: array) -> None:
        """ä½¿ç”¨çŸ©é˜µè®¡ç®—å¾—å‡ºæ™®é€šæœ€å°äºŒä¹˜æ³•çš„æˆªè·é¡¹å’Œç³»æ•°
        :param x_train_design: (n_samples, n_features + 1), è®­ç»ƒæ•°æ®æ·»åŠ ä¸€åˆ—å¸¸æ•°é¡¹ï¼ˆè®¾è®¡çŸ©é˜µï¼‰
        :param y_train: (n_samples, 1), è®­ç»ƒæ•°æ®çš„å€¼
        :exception  å¦‚æžœæ— æ³•è®¡ç®—é€†çŸ©é˜µï¼Œåˆ™æ•èŽ·LinAlgErrorå¼‚å¸¸ï¼Œç„¶åŽè®¡ç®—ä¼ªé€†çŸ©é˜µ
        """
        x_train_square = x_train_design.T @ x_train_design  # A @ B == np.dot(A, B)
        try:
            inverse = inv(x_train_square)
        except LinAlgError as e:
            print(e)
            inverse = pinv(x_train_square)
        self.theta = inverse @ x_train_design.T @ y_train

    @staticmethod
    def gradient_descent_batch_func(*args):
        """æ‰¹é‡æ¢¯åº¦ä¸‹é™æ›´æ–°ç³»æ•°æ–¹æ³•"""
        x_train_design, y_train, theta, alpha, _ = args
        n_samples = x_train_design.shape[0]
        y_train_pred = x_train_design @ theta
        return theta - 1 / n_samples * alpha * x_train_design.T @ (y_train_pred - y_train)

    @staticmethod
    def gradient_descent_mini_batch_func(*args):
        """
        å°æ‰¹é‡/éšæœº æ¢¯åº¦ä¸‹é™æ›´æ–°ç³»æ•°æ–¹æ³•
        å°†batch_sizeè®¾ä¸º1ï¼Œå³ä¸ºéšæœºæ¢¯åº¦ä¸‹é™ï¼Œå¤§äºŽ1å³ä¸ºå°æ‰¹é‡æ¢¯åº¦ä¸‹é™
        """
        x_train_design, y_train, theta, alpha, batch_size = args
        n_samples = x_train_design.shape[0]
        if batch_size >= n_samples:
            raise ValueError(f"Param batch_size[{batch_size}] should < n_samples[{n_samples}]")
        random_indices = np.random.choice(x_train_design.shape[0], size=batch_size, replace=False)
        x_train_sub, y_train_sub = x_train_design[random_indices], y_train[random_indices]
        y_train_pred = x_train_sub @ theta
        return theta - 1 / batch_size * alpha * x_train_sub.T @ (y_train_pred - y_train_sub)

    def predict(self, x_test, designed: bool = False):
        """é¢„æµ‹
        :param x_test: æµ‹è¯•æ•°æ®
        :param designed: æµ‹è¯•æ•°æ®æ˜¯å¦æ˜¯è®¾è®¡çŸ©é˜µï¼ˆå³æ˜¯å¦å·²ç»æ·»åŠ ä¸€åˆ—å¸¸æ•°åˆ—ï¼‰
        :return (n_samples,), æµ‹è¯•æ•°æ®çš„é¢„æµ‹å€¼
        """
        x_test_design = x_test if designed else transform_array(x_test)
        return x_test_design @ self.theta

    def plot_losses(self):
        """ç»˜åˆ¶è¿­ä»£è¿‡ç¨‹ä¸­çš„æŸå¤±å˜åŒ–æƒ…å†µ"""
        if self.losses is None:
            raise NotFittedWithGradientDescentError()
        plt.plot(range(1, self.losses.shape[0] + 1), self.losses)
        plt.title('Loss changes with the number of iterations')
        plt.xlabel('number of iter')
        plt.ylabel('current loss')
        plt.show()
