

<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->
<!-- code_chunk_output -->

* [时间序列有什么特别之处](#时间序列有什么特别之处)
* [在pandas上加载时间序列](#在pandas上加载时间序列)
* [如何检验时间序列的稳定性](#如何检验时间序列的稳定性)
* [如何令时间序列稳定](#如何令时间序列稳定)
	* [消除趋势](#消除趋势)
	* [移动平均数](#移动平均数)
	* [消除趋势和季节性](#消除趋势和季节性)
		* [差分](#差分)
		* [分解](#分解)
* [时间序列预测](#时间序列预测)
	* [ARIMA](#arima)
		* [自回归AR模型](#自回归ar模型)
		* [组合模型](#组合模型)
		* [移动平均数（MA ）模型](#移动平均数ma-模型)
	* [倒回到原始区间](#倒回到原始区间)

<!-- /code_chunk_output -->

# 时间序列有什么特别之处
时间序列是时间间隔不变的情况下收集的时间点集合。这些集合被分析用来了解长期发展趋势，为了预测未来或者表现分析的其他形式

为什么时间序列与常见的回归问题不同》
- 时间序列与时间相关。基于线性回归的假设是观察结果是独立的，这种情况与时间序列不相符。
- 随着上升或下降的趋势，更多的时间序列出现季节性趋势的形式。
# 在pandas上加载时间序列
1、import相关包以及相关设置
```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.pylab import rcParams
rcParams['figure.figsize'] = 15, 6
```

2、加载本地csv数据为DataFrame
```python
data = pd.read_csv('Machine_Learning\时间序列分析\AirPassengers.csv')
print(data.head())
print("data的数据类型")
print(data.dtypes)
```
```
Month  #Passengers
0  1949-01          112
1  1949-02          118
2  1949-03          132
3  1949-04          129
4  1949-05          121

data的数据类型
Month          object
#Passengers     int64
dtype: object
```
数据包含了指定的月份和该月的游客数量。但是时间序列对象的读取和数据类型的`object`和`int`类型的读取是不一样的，需要设置`parse_dates`参数
```python
# 日期解析器
dateparse = lambda date: pd.datetime.strptime(date, '%Y-%m')
# 重新以新的解析方法dateparse来读取日期字段
data = pd.read_csv('Machine_Learning\时间序列分析\AirPassengers.csv', parse_dates=['Month'], index_col='Month', date_parser=dateparse)
print(data.head(5))
```
```
                    #Passengers
Month                  
1949-01-01          112
1949-02-01          118
1949-03-01          132
1949-04-01          129
1949-05-01          121
```
我们逐个解释下这些参数：
- parse_dates: 指定含有时间信息的字段
- index_col: 将某一列直接作为DataFrame的索引，这里就是将日期作为索引
- date_parser: 解析日期的函数

现在查看一下数据的索引：
```python
data.index
```
```
DatetimeIndex(['1949-01-01', '1949-02-01', '1949-03-01', '1949-04-01',
               '1949-05-01', '1949-06-01', '1949-07-01', '1949-08-01',
               '1949-09-01', '1949-10-01',
               ...
               '1960-03-01', '1960-04-01', '1960-05-01', '1960-06-01',
               '1960-07-01', '1960-08-01', '1960-09-01', '1960-10-01',
               '1960-11-01', '1960-12-01'],
              dtype='datetime64[ns]', name='Month', length=144, freq=None)

```

**注意：** `dtype='datetime64[ns]'`表明它确实是一个时间数据对象。
```python
ts = data['#Passengers']
ts.head(5)
```
```
Month
1949-01-01    112
1949-02-01    118
1949-03-01    132
1949-04-01    129
1949-05-01    121
Name: #Passengers, dtype: int64
```

在进一步深入之前，探讨一下关于时间序列数据的索引技术。可以通过两种方式实现：
```python
ts['1949-01-01']  # 112
from datetime import datetime
ts[datetime(1949,1,1)]  # 112
```

两种方法都可以返回值`112`。切片亦如此：
```python
ts['1949-01-01':'1949-05-01']
```
```
Month
1949-01-01    112
1949-02-01    118
1949-03-01    132
1949-04-01    129
1949-05-01    121
Name: #Passengers, dtype: int64
```
```python
ts[datetime(1949,1,1):datetime(1949,5,1)]
```
```
Month
1949-01-01    112
1949-02-01    118
1949-03-01    132
1949-04-01    129
1949-05-01    121
Name: #Passengers, dtype: int64
```

**注意：**
- 与数值索引不一样，结束索引在这里是被包含的
- 索引不能被随意打乱

另一个实用例子：索引出1949年所有的值：
```python
ts['1949']
```
```
Month
1949-01-01    112
1949-02-01    118
1949-03-01    132
1949-04-01    129
1949-05-01    121
1949-06-01    135
1949-07-01    148
1949-08-01    148
1949-09-01    136
1949-10-01    119
1949-11-01    104
1949-12-01    118
Name: #Passengers, dtype: int64
```
月份部分已经省略。如果要获取某月所有天的值，日期部分也可省略。

# 如何检验时间序列的稳定性
**如果一个时间序列的统计特征如平均数、方差随着时间保持不变，我们认为它是稳定的**

稳定性的标准：
- 平均值不变
- 方差不变
- 自协方差不随着时间变化

```python
ts.plot()
```
![1](https://i.loli.net/2019/05/24/5ce74a80b39e799477.png)

可以清晰地看到，随着季节性的变动，飞机乘客的数量总体不断增长。

但是不是经常都可以获得这样清晰的视觉推论。所以，更正式的，我们可以通过下面的方法测试稳定性。
- 绘制滚动统计：绘制移动平均数和移动方差，观察其是否随着时间变化。
- DF检验：这是一种检查数据稳定性的统计测试。无效假设：时间序列是不稳定的。测试结果由测试统计量和置信区间的临界值组成。如果测试统计量小于临界值，我们可以拒绝无效假设，认为时间序列是稳定的。

```python
from statsmodels.tsa.stattools import adfuller

def test_stationarity(timeseries):
    """测试平稳性"""
    # 滚动统计数据
    rolmean = timeseries.rolling(12).mean()
    # rolmean = pd.rolling_mean(timeseries, window=12)  # 滚动平均
    rolstd = timeseries.rolling(12).std()
    # rolstd = pd.rolling_std(timeseries, window=12)  # 滚动标准差(用标准差代替方差)

    # 绘制滚动统计量的图表
    plt.plot(timeseries, color='blue', label='Original')
    plt.plot(rolmean, color='red', label='Rolling Mean')
    plt.plot(rolstd, color='black', label='Rolling Std')
    plt.legend(loc='best')
    plt.title("Rolling Mean & Standard Deviation")
    plt.show()

    # 执行DF(Dickey-Fuller)测试
    print("DF测试的结果如下：")
    dftest = adfuller(timeseries, autolag='AIC')
    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])

    for key, value in dftest[4].items():
        dfoutput['Critical Value (%s)' % key] = value
    print(dfoutput)
test_stationarity(ts)
```
![2](https://i.loli.net/2019/05/24/5ce74e7a6acc637669.png)
```
DF测试的结果如下：
Test Statistic                   0.815369
p-value                          0.991880
#Lags Used                      13.000000
Number of Observations Used    130.000000
Critical Value (1%)             -3.481682
Critical Value (5%)             -2.884042
Critical Value (10%)            -2.578770
dtype: float64
```

基本上是不可能使序列完全稳定，我们只能努力让它尽可能的稳定。

先让我们弄明白是什么导致时间序列不稳定。这儿有两个主要原因:
- 趋势-随着时间产生不同的平均值。举例：在飞机乘客这个案例中，我们看到总体上，飞机乘客的数量是在不断增长的。
- 季节性-特定时间框架内的变化。举例：在特定的月份购买汽车的人数会有增加的趋势，因为车价上涨或者节假日到来。

模型的根本原理是预测序列的趋势和季节性，从序列中删除这些因素，将得到一个稳定的序列。然后统计预测技术可以在这个序列上完成。最后一步是通过运用趋势和季节性限制倒回到将预测值转换成原来的区间。
# 如何令时间序列稳定
## 消除趋势
消除趋势的第一个方法是转换。例如,在本例中,我们可以清楚地看到,有一个显著的趋势。所以我们可以通过变换，惩罚较高值而不是较小值。这可以采用日志,平方根,立方跟等等。让我们简单在这儿转换一个对数。
```python
ts_log = np.log(ts)
ts_log.plot()
```

![3](https://i.loli.net/2019/05/24/5ce7502d32aa484137.png)

取对数对这个方法来说不是太直观。所以我们可以再进一步采用一些技术来估计或者对这个趋势建模，然后将其从序列中删除。这里有很多方法，最常见的有：
- 聚合：取一段时间的平均值(月/周平均值)
- 平滑：取滚动平均值
- 多项式回归分析

我在这儿讨论将平滑,你也应该尝试其他可以解决的问题的技术。平滑是指采取滚动估计,即考虑过去的几个实例。有各种方法可以解决这些问题，但我将主要讨论以下两个：
- 移动平均数
- 消除趋势和季节性
## 移动平均数
在这个方法中，根据时间序列的频率采用k连续值的平均数。我们可以采用过去一年的平均数，即过去12个月的平均数。关于确定滚动数据，pandas有特定的功能定义。
```python
moving_avg = ts_log.rolling(12).mean()
ts_log.plot()
moving_avg.plot()
```
![4](https://i.loli.net/2019/05/24/5ce7524dac07b65041.png)

红色表示了滚动平均数。让我们从原始序列中减去这个平均数。**注意**，从我们采用过去12个月的值开始，滚动平均法还没有对前11个月的值定义。我们可以看到：
```python
ts_log_moving_avg_diff = ts_log - moving_avg
ts_log_moving_avg_diff.head(12)
```
```
Month
1949-01-01         NaN
1949-02-01         NaN
1949-03-01         NaN
1949-04-01         NaN
1949-05-01         NaN
1949-06-01         NaN
1949-07-01         NaN
1949-08-01         NaN
1949-09-01         NaN
1949-10-01         NaN
1949-11-01         NaN
1949-12-01   -0.065494
Name: #Passengers, dtype: float64
```
注意前11个月是NaN，因为计算移动平均需要12个月的数据。现在删掉这11个月的数据然后检查这些模块以测试稳定性。
```python
ts_log_moving_avg_diff.dropna(inplace=True)
test_stationarity(ts_log_moving_avg_diff)
```
![5](https://i.loli.net/2019/05/24/5ce7541865a8055680.png)

```
DF测试的结果如下：
Test Statistic                  -3.162908
p-value                          0.022235
#Lags Used                      13.000000
Number of Observations Used    119.000000
Critical Value (1%)             -3.486535
Critical Value (5%)             -2.886151
Critical Value (10%)            -2.579896
dtype: float64
```
这看起来像个更好的序列。滚动平均值出现轻微的变化，但是没有明显的趋势。同时，检验统计量比5%的临界值小，所以我们在95%的置信区间认为它是稳定序列。

但是，这个方法有一个缺陷：要严格定义时段。在这种情况下，我们可以采用年平均数，但是对于复杂的情况的像预测股票价格，是很难得到一个数字的。所以，我们采取**加权移动平均法**可以对最近的值赋予更高的权重。

**指数加权移动平均法** 是很受欢迎的方法，所有的权重被指定给先前的值连同衰减系数。这可以通过pandas实现：
```python

expwighted_avg = pd.DataFrame(ts_log).ewm(12).mean()
plt.plot(ts_log)
plt.plot(expwighted_avg)
```
![6](https://i.loli.net/2019/05/24/5ce756dcc60e294671.png)

注意,这里使用了参数“半衰期”来定义指数衰减量。这只是一个假设,将很大程度上取决于业务领域。其他参数,如跨度和质心也可以用来定义衰减。

现在让我们从这个序列转移，继续检查稳定性。
```python
ts_log_ewma_diff = ts_log - expwighted_avg['#Passengers']
test_stationarity(ts_log_ewma_diff)
```
![7](https://i.loli.net/2019/05/24/5ce757a97a1cf76992.png)
```
DF测试的结果如下：
Test Statistic                  -3.566092
p-value                          0.006443
#Lags Used                      13.000000
Number of Observations Used    130.000000
Critical Value (1%)             -3.481682
Critical Value (5%)             -2.884042
Critical Value (10%)            -2.578770
dtype: float64
```

这个时间序列有更少的平均值变化和标准差大小变化。同时，检验统计量小于1%的临界值,这比以前的情况好。请注意在这种情况下就不会有遗漏值因为所有的值在一开始就被赋予了权重。所以在运行的时候，它没有先前的值参与。


## 消除趋势和季节性
之前讨论来了简单的趋势减少技术不能在所有情况下使用，特别是在高季节性情况下。让我们谈论一下两种消除趋势和季节性的方法。
- 差分-采用一个特定时间差的差值
- 分解-建立有关趋势和季节性的模型，然后从模型中删除它们

### 差分
处理趋势和季节性的最常见的方法之一就是差分法。在这种方法中,我们采用特定瞬间和它前一个瞬间的不同的观察结果。这主要是在提高平稳性。
pandas可以实现一阶差分：
```python
ts_log_diff = ts_log - ts_log.shift(1)
ts_log_diff.plot()
```
表中可以看出很大程度上减少了趋势。让我们通过模块验证一下：
```python
ts_log_diff.dropna(inplace=True)
test_stationarity(ts_log_diff)
```
![9](https://i.loli.net/2019/05/24/5ce758cc21eca59697.png)

```
DF测试的结果如下：
Test Statistic                  -2.717131
p-value                          0.071121
#Lags Used                      14.000000
Number of Observations Used    128.000000
Critical Value (1%)             -3.482501
Critical Value (5%)             -2.884398
Critical Value (10%)            -2.578960
dtype: float64
```

![8](https://i.loli.net/2019/05/24/5ce75855da31297609.png)

我们可以看到平均数和标准差随着时间有小的变化。同时，DF检验统计量小于10% 的临界值，因此该时间序列在90%的置信区间上是稳定的。我们同样可以采取二阶或三阶差分在具体应用中获得更好的结果。这些方法你可以自己尝试。

### 分解
在这种方法中,趋势和季节性是分别建模的并倒回到序列的保留部分。我将跳过统计数据，直接给出结果:
```python
from statsmodels.tsa.seasonal import seasonal_decompose
decomposition = seasonal_decompose(ts_log)

trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid

plt.subplot(411)
plt.plot(ts_log, label='Original')
plt.legend(loc='best')
plt.subplot(412)
plt.plot(trend, label='Trend')
plt.legend(loc='best')
plt.subplot(413)
plt.plot(seasonal,label='Seasonality')
plt.legend(loc='best')
plt.subplot(414)
plt.plot(residual, label='Residuals')
plt.legend(loc='best')
plt.tight_layout()
```

![10](https://i.loli.net/2019/05/24/5ce759d872ed555585.png)


在这里我们可以看到趋势,季节性从数据分离，我们可以建立残差的模型，让我们检查残差的稳定性：

```python
ts_log_decompose = residual
ts_log_decompose.dropna(inplace=True)
test_stationarity(ts_log_decompose)
```
![11](https://i.loli.net/2019/05/24/5ce75a61e50b574188.png)
```
DF测试的结果如下：
Test Statistic                -6.332387e+00
p-value                        2.885059e-08
#Lags Used                     9.000000e+00
Number of Observations Used    1.220000e+02
Critical Value (1%)           -3.485122e+00
Critical Value (5%)           -2.885538e+00
Critical Value (10%)          -2.579569e+00
dtype: float64
```

DF测试统计量明显低于1%的临界值，这样时间序列是非常接近稳定。你也可以尝试高级的分解技术产生更好的结果。同时,你应该注意到, 在这种情况下将残差转换为原始值对未来数据不是很直观。


# 时间序列预测
我们看到不同的技术和它们有效的工作使得时间序列得以稳定。让我们建立差分后的时间序列模型，因为它是很受欢迎的技术，也相对更容易添加噪音和季节性倒回到预测残差。在执行趋势和季节性评估技术上,有两种情况:
- 不含依赖值的严格稳定系列。简单的情况下,我们可以建立残差模型作为白噪音（指功率谱密度在整个频域内均匀分布的噪声）。但这是非常罕见的。
- 序列含有明显的依赖值。在这种情况下,我们需要使用一些统计模型像ARIMA（差分自回归移动平均模型）来预测数据。

## ARIMA
让我给你简要介绍一下ARIMA，我不会介绍技术细节,但如果你希望更有效地应用它们，你应该理解这些概念的细节。ARIMA代表自回归整合移动平均数。平稳时间序列的ARIMA预测的只不过是一个线性方程(如线性回归)。预测依赖于ARIMA模型参数(p d q)。
- 自回归函数AR的条件p：AR条件是因变量的滞后。如：如果p=5，那么预测x(t)将是x(t-1),x(t-2),...,x(t-5)
- 移动平均数MA的条件q：MA条件是预测方程的滞后预测误差。如：如果q=5，预测x(t)将是e(t-1),e(t-2),...,e(t-5)。e(i)是移动平均数在第i个瞬间和实际值的差值
- 差分d：有非季节性的差值，即这种情况下我们采用一阶差分。所以传递变量，令d=0或者传递原始变量，令d=1。两种方法得到的结果一样。

在这里一个重要的问题是如何确定“p”和“q”的值。我们使用两个坐标来确定这些数字。我们来讨论它们。
- 自相关函数ACF：这是时间序列和它自身滞后之间的相关性的测试。如在自相关函数可以比较时间的瞬间‘t1’…’t2’以及序列的瞬间‘t1-5’…’t2-5’ (t1-5和t2 是结束点)。
- 部分自相关函数PACF：这是时间序列和它自身滞后版本之间的相关性测试，但是是在预测（已经通过比较干预得到解释）的变量后。如：滞后值为5，它将检查相关性，但是会删除从滞后值1到4得到的结果。

时间序列的自回归函数和部分自回归函数可以在差分后绘制为:

```python
from statsmodels.tsa.stattools import acf, pacf

lag_acf = acf(ts_log_diff, nlags=20)
lag_pacf = pacf(ts_log_diff, nlags=20, method='ols')

plt.subplot(121)
plt.plot(lag_acf)
plt.axhline(y=0,linestyle='--',color='gray')
plt.axhline(y=-1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')
plt.axhline(y=1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')
plt.title('Autocorrelation Function')

plt.subplot(122)
plt.plot(lag_pacf)
plt.axhline(y=0,linestyle='--',color='gray')
plt.axhline(y=-1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')
plt.axhline(y=1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')
plt.title('Partial Autocorrelation Function')
plt.tight_layout()
```

![12](https://i.loli.net/2019/05/24/5ce7612be916f61396.png)


在这个点上,0的每一条边上的两条虚线之间是置信区间。这些可以用来确定“p”和“q”的值:

1、p-部分自相关函数表第一次截断的上层置信区间是滞后值。如果你仔细看，该值是p=2。

2、q- 自相关函数表第一次截断的上层置信区间是滞后值。如果你仔细看，该值是q=2。

现在，考虑个体以及组合效应建立3个不同的ARIMA模型

```python
from statsmodels.tsa.arima_model import ARIMA
```
p,d,q值可以指定使用ARIMA的命令参数即采用一个元(p,d,q)。建立三种情况下的模型：

### 自回归AR模型
```python
model = ARIMA(ts_log, order=(2, 1, 0))  
results_AR = model.fit(disp=-1)  
plt.plot(ts_log_diff)
plt.plot(results_AR.fittedvalues, color='red')
plt.title('RSS: %.4f'% sum((results_AR.fittedvalues-ts_log_diff)**2))
```
![13](https://i.loli.net/2019/05/24/5ce7627b1784796972.png)
```
Text(0.5, 1.0, 'RSS: 1.5023')
```
### 组合模型
```python
model = ARIMA(ts_log, order=(0, 1, 2))  
results_MA = model.fit(disp=-1)  
plt.plot(ts_log_diff)
plt.plot(results_MA.fittedvalues, color='red')
plt.title('RSS: %.4f'% sum((results_MA.fittedvalues-ts_log_diff)**2))
```
```
Text(0.5, 1.0, 'RSS: 1.4721')
```
![14](https://i.loli.net/2019/05/24/5ce762b702dea78646.png)

### 移动平均数（MA ）模型
```python
model = ARIMA(ts_log, order=(2, 1, 2))  
results_ARIMA = model.fit(disp=-1)  
plt.plot(ts_log_diff)
plt.plot(results_ARIMA.fittedvalues, color='red')
plt.title('RSS: %.4f'% sum((results_ARIMA.fittedvalues-ts_log_diff)**2))
```
```
Text(0.5, 1.0, 'RSS: 1.0292')
```
![15](https://i.loli.net/2019/05/24/5ce762f060b9543901.png)

在这里我们可以看到,自回归函数模型和移动平均数模型几乎有相同的RSS，但相结合效果显著更好。现在,我们只剩下最后一步,即把这些值倒回到原始区间。
## 倒回到原始区间
既然组合模型获得更好的结果，让我们将它倒回原始值，看看它如何执行

第一步是作为一个独立的序列，存储预测结果,观察它。
```python
predictions_ARIMA_diff = pd.Series(results_ARIMA.fittedvalues, copy=True)
predictions_ARIMA_diff.head(5)
```
```
Month
1949-02-01    0.009580
1949-03-01    0.017491
1949-04-01    0.027670
1949-05-01   -0.004521
1949-06-01   -0.023889
dtype: float64
```
注意,这些是从‘1949-02-01’开始,而不是第一个月。为什么?这是因为我们将第一个月份取为滞后值，一月前面没有可以减去的元素。

将差分转换为对数尺度的方法是这些差值连续地添加到基本值。一个简单的方法就是首先确定索引的累计总和,然后将其添加到基本值。累计总和可以在下面找到:
```python
predictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()
predictions_ARIMA_diff_cumsum.head()
```
```
Month
1949-02-01    0.009580
1949-03-01    0.027071
1949-04-01    0.054742
1949-05-01    0.050221
1949-06-01    0.026331
dtype: float64
```

接下来我们将它们添加到基本值。为此我们将使用所有的值创建一个序列作为基本值，并添加差值。我们这样做：

```python
predictions_ARIMA_log = pd.Series(ts_log.ix[0], index=ts_log.index)
predictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum,fill_value=0)
predictions_ARIMA_log.head()
```
```
Month
1949-01-01    4.718499
1949-02-01    4.728079
1949-03-01    4.745570
1949-04-01    4.773241
1949-05-01    4.768720
dtype: float64
```

第一个元素是基本值本身，从基本值开始值累计添加。最后一步是将指数与原序列比较。

```python
predictions_ARIMA = np.exp(predictions_ARIMA_log)
plt.plot(ts)
plt.plot(predictions_ARIMA)
plt.title('RMSE: %.4f'% np.sqrt(sum((predictions_ARIMA-ts)**2)/len(ts)))
```
```
Text(0.5, 1.0, 'RMSE: 90.1045')
```
![16](https://i.loli.net/2019/05/24/5ce765babb06e72827.png)

最后我们获得一个原始区间的预测结果。虽然不是一个很好的预测。但是你获得了思路对吗?现在,我把它留个你去进一步改进，做一个更好的方案。
